# -*- coding: utf-8 -*-
"""Image_Classification_with_CNNs_Upgraded.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cfKY8gVmLNoiBESSnz-O1_fTK1lu6U_J

# Image Classification with CNNs â€” Endâ€‘toâ€‘End Project (Upgraded)
This notebook satisfies the course requirements for **Project Path B: Image Classification with CNNs**. It includes data cleaning, EDA, feature engineering, CNN + traditional ML models, hyperparameter tuning, model persistence (pickle), and an exportable metrics JSON for the web app.

**Tip:** Run this in Google Colab with GPU enabled (Runtime â†’ Change runtime type â†’ GPU).
"""

# =============================
# Environment Setup (Colab-safe)
# =============================

!command -v nvidia-smi >/dev/null && nvidia-smi -L || echo "âš ï¸ No GPU detected. Tip: Runtime > Change runtime type > GPU"

# Clean conflicts & install compatible versions
!pip uninstall -y sklearn-compat tensorflow-decision-forests || true

# Install compatible versions for TensorFlow 2.19.0
!pip install -U \
    tensorflow==2.19.0 tensorflow-text==2.19.0 tf-keras==2.19.0 \
    scikit-learn==1.3.2 sklearn-compat==0.1.3 imbalanced-learn==0.13.0 \
    joblib pillow matplotlib opencv-python seaborn plotly

import os, json, random, shutil, glob, pickle
from pathlib import Path

import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline as SkPipeline
from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,
                             precision_recall_fscore_support, ConfusionMatrixDisplay)
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression

import joblib
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.applications.efficientnet import preprocess_input as effnet_preprocess

SEED = 42
random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)

# Project dirs
WORK_DIR = "/content/brain_mri_work"  # change to your dataset project folder if needed
for p in ["raw","csv","clean","eda","features","models","metrics","app"]:
    Path(WORK_DIR, p).mkdir(parents=True, exist_ok=True)

print("TensorFlow:", tf.__version__)

"""## Step 1 â€” Dataset Selection & Config
Set `DATA_DIR` to your **image root folder** that contains one subfolder per class:
```
DATA_DIR/
  â”œâ”€â”€ class_a/
  â”œâ”€â”€ class_b/
  â””â”€â”€ class_c/
```
Requirement: at least **5,000 images** total, with **real cleaning needs** (mislabeled/corrupt/noisy).
"""

import zipfile
import os

# Path to the uploaded file
zip_path = '/content/archive (2) (1).zip'
extract_dir = '/mnt/data/extracted_files'

# Create directory for extraction if it doesn't exist
os.makedirs(extract_dir, exist_ok=True)

# Extract the ZIP file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# List the extracted files
extracted_files = os.listdir(extract_dir)
extracted_files

import zipfile
from pathlib import Path

# Path where your uploaded ZIP is stored in Colab
zip_path = "/content/archive (2) (1).zip"  # Rename if needed

# Extract destination
extract_dir = Path("/content/data_images")
extract_dir.mkdir(parents=True, exist_ok=True)

# Unzip
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print(f"âœ… Extracted to: {extract_dir}")

# Now set DATA_DIR to this location
DATA_DIR = extract_dir
assert DATA_DIR.exists(), f"âŒ DATA_DIR does not exist: {DATA_DIR}"

from pathlib import Path

# Point to your extracted dataset root
DATA_DIR = Path("/content/data_images")  # Update if stored elsewhere
assert DATA_DIR.exists(), f"âŒ DATA_DIR does not exist: {DATA_DIR}"

IMG_H, IMG_W = 224, 224
BATCH_SIZE   = 32
EPOCHS       = 20
VAL_SPLIT    = 0.2
TEST_SPLIT   = 0.1

def scan_images(root: Path):
    classes, filepaths, labels = [], [], []
    for cls in sorted([d for d in root.iterdir() if d.is_dir()]):
        imgs = sorted([str(p) for p in cls.rglob("*") if p.suffix.lower() in [".jpg",".jpeg",".png",".bmp",".tif",".tiff"]])
        if len(imgs) > 0:
            classes.append(cls.name)
            filepaths.extend(imgs)
            labels.extend([cls.name] * len(imgs))
    return classes, filepaths, labels

classes, filepaths, labels = scan_images(DATA_DIR)

print(f"âœ… Found {len(classes)} classes")
print(f"âœ… Total images: {len(filepaths)}")
print(f"ðŸ“‚ First 5 classes: {classes[:5]}")

"""## Step 2 â€” Data Cleaning
- Handle corrupted/unreadable files
- Standardize size & channels
- Normalize pixel values
- (Optional) flag potential mislabels by simple heuristics (e.g., perceptual hash outliers)
"""

import cv2

def is_corrupt(path):
    try:
        img = cv2.imdecode(np.fromfile(path, dtype=np.uint8), cv2.IMREAD_COLOR)
        return img is None
    except Exception:
        return True

# Filter out corrupt images
good_paths, good_labels, bad_paths = [], [], []
for p, y in zip(filepaths, labels):
    if is_corrupt(p):
        bad_paths.append(p)
    else:
        good_paths.append(p); good_labels.append(y)

print(f"Corrupt images removed: {len(bad_paths)}")
filepaths, labels = good_paths, good_labels

# Encode labels
le = LabelEncoder()
y_all = le.fit_transform(labels)
class_names = list(le.classes_)
print('Classes:', class_names)
with open(Path(WORK_DIR,"csv","label_encoder.pkl"), "wb") as f:
    pickle.dump(le, f)

"""## Step 3 â€” EDA
- Sample images per class
- Class distribution
- Image characteristics (brightness)
- Save EDA assets to `/eda`
"""

from collections import Counter
import matplotlib.pyplot as plt

# Class distribution
cnt = Counter(labels)
dist = pd.DataFrame({"class": list(cnt.keys()), "count": list(cnt.values())}).sort_values("count", ascending=False)
dist.to_csv(Path(WORK_DIR,"csv","class_distribution.csv"), index=False)

plt.figure(figsize=(10,4))
plt.xticks(rotation=45, ha='right')
plt.bar(dist["class"], dist["count"])
plt.title("Class Distribution")
plt.tight_layout()
plt.savefig(Path(WORK_DIR,"eda","class_distribution.png"))
plt.show()

# Sample grid
def show_samples_per_class(paths, lbls, n_per_class=3):
    fig, axes = plt.subplots(len(class_names), n_per_class, figsize=(n_per_class*3, len(class_names)*3))
    if len(class_names)==1:
        axes = np.array([axes])
    for i, cls in enumerate(class_names):
        idxs = [k for k,(p,y) in enumerate(zip(paths,lbls)) if y==cls][:n_per_class]
        for j, k in enumerate(idxs):
            img = Image.open(paths[k]).convert("RGB").resize((IMG_W, IMG_H))
            axes[i,j].imshow(img)
            axes[i,j].axis("off")
            axes[i,j].set_title(cls if j==0 else "")
    plt.tight_layout()
    plt.savefig(Path(WORK_DIR,"eda","samples_grid.png"))
    plt.show()

if len(filepaths)>0:
    show_samples_per_class(filepaths, labels, n_per_class=3)

# Brightness stats
def image_brightness(path):
    img = Image.open(path).convert("L").resize((IMG_W, IMG_H))
    return np.array(img).mean()

bright = [image_brightness(p) for p in filepaths[:2000]]  # sample to speed up
pd.Series(bright).to_csv(Path(WORK_DIR,"csv","brightness_sample.csv"), index=False)
plt.figure()
plt.hist(bright, bins=30)
plt.title("Brightness Distribution (sample)")
plt.savefig(Path(WORK_DIR,"eda","brightness_hist.png"))
plt.show()

"""## Step 4 â€” Feature Engineering / Preprocessing
We build TensorFlow datasets with **augmentation** (rotation/flip/zoom) for training and normalization.
"""

# Train/Val/Test split
X_temp, X_test, y_temp, y_test = train_test_split(filepaths, y_all, test_size=TEST_SPLIT, stratify=y_all, random_state=SEED)
val_ratio = VAL_SPLIT / (1.0 - TEST_SPLIT)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_ratio, stratify=y_temp, random_state=SEED)

print(len(X_train), len(X_val), len(X_test))

AUTOTUNE = tf.data.AUTOTUNE

def load_and_preprocess(path, label):
    img = tf.io.read_file(path)
    img = tf.io.decode_image(img, channels=3, expand_animations=False)
    img = tf.image.resize(img, [IMG_H, IMG_W])
    img = tf.cast(img, tf.float32)/255.0
    return img, label

augment = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.05),
    layers.RandomZoom(0.1),
])

def make_ds(paths, labels, augment_on=False, shuffle=False):
    ds = tf.data.Dataset.from_tensor_slices((paths, labels))
    ds = ds.map(load_and_preprocess, num_parallel_calls=AUTOTUNE)
    if augment_on:
        ds = ds.map(lambda x,y: (augment(x, training=True), y), num_parallel_calls=AUTOTUNE)
    if shuffle:
        ds = ds.shuffle(2048, seed=SEED)
    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)
    return ds

train_ds = make_ds(X_train, y_train, augment_on=True,  shuffle=True)
val_ds   = make_ds(X_val,   y_val,   augment_on=False, shuffle=False)
test_ds  = make_ds(X_test,  y_test,  augment_on=False, shuffle=False)

"""## Step 5 â€” CNN Model (Custom)
We use a lightweight custom CNN and train with a learning-rate scheduler and model checkpoint.
Training runs **all EPOCHS**; we reload the **best weights** afterwards.
"""

def build_custom_cnn(num_classes):
    inputs = layers.Input(shape=(IMG_H, IMG_W, 3))
    x = layers.Conv2D(32, 3, activation='relu')(inputs)
    x = layers.MaxPooling2D()(x)
    x = layers.Conv2D(64, 3, activation='relu')(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Conv2D(128, 3, activation='relu')(x)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)
    model = models.Model(inputs, outputs)
    return model

num_classes = len(class_names) if len(class_names)>0 else 2
custom = build_custom_cnn(num_classes)
custom.compile(optimizer=tf.keras.optimizers.Adam(1e-3),
               loss='sparse_categorical_crossentropy',
               metrics=['accuracy'])

ckpt_dir = Path(WORK_DIR,"models","cnn_custom"); ckpt_dir.mkdir(parents=True, exist_ok=True)
cbs = [
    tf.keras.callbacks.ModelCheckpoint(filepath=str(ckpt_dir / "best.keras"),
                                       monitor='val_loss', save_best_only=True),
    tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5, monitor='val_loss')
]

hist_custom = custom.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=cbs, verbose=1)
custom.load_weights(str(ckpt_dir / "best.keras"))
custom.save(ckpt_dir / "saved_model.keras")

# Evaluate on test
test_loss, test_acc = custom.evaluate(test_ds, verbose=0)
print(f"[CNN] Test accuracy: {test_acc:.4f}")

"""## Step 5b â€” Feature Extraction for Traditional ML Models
We extract features from a pre-trained **EfficientNetB0** and train SVM, RandomForest, KNN, and LogisticRegression.
"""

effnet_base = EfficientNetB0(weights='imagenet', include_top=False, pooling=None)

def build_infer_ds(filepaths, img_size=(IMG_H, IMG_W), batch=BATCH_SIZE):
    labels_dummy = np.zeros(len(filepaths), dtype=np.int64)
    def _load_and_decode(path, _):
        img = tf.io.read_file(path)
        img = tf.io.decode_image(img, channels=3, expand_animations=False)
        img = tf.image.resize(img, img_size)
        img = tf.cast(img, tf.float32)/255.0
        return img, 0
    ds = tf.data.Dataset.from_tensor_slices((filepaths, labels_dummy))
    ds = ds.map(_load_and_decode, num_parallel_calls=tf.data.AUTOTUNE)
    return ds.batch(batch).prefetch(tf.data.AUTOTUNE)

def extract_features_with_base(base, filepaths, img_size=(IMG_H, IMG_W), batch=BATCH_SIZE):
    ds = build_infer_ds(filepaths, img_size, batch)
    feats, gap = [], tf.keras.layers.GlobalAveragePooling2D()
    for batch_imgs, _ in ds:
        x = effnet_preprocess(batch_imgs)
        f = base(x, training=False)
        f = gap(f).numpy()
        feats.append(f)
    return np.vstack(feats)

print("[FEATS] Extracting featuresâ€¦")
Xtr = extract_features_with_base(effnet_base, X_train)
Xv  = extract_features_with_base(effnet_base, X_val)
Xte = extract_features_with_base(effnet_base, X_test)

np.save(Path(WORK_DIR,"features","X_train.npy"), Xtr); np.save(Path(WORK_DIR,"features","y_train.npy"), y_train)
np.save(Path(WORK_DIR,"features","X_val.npy"),   Xv);  np.save(Path(WORK_DIR,"features","y_val.npy"),   y_val)
np.save(Path(WORK_DIR,"features","X_test.npy"),  Xte); np.save(Path(WORK_DIR,"features","y_test.npy"),  y_test)

"""## Step 6 â€” Traditional ML Models & Hyperparameter Tuning
We train SVM, RandomForest, KNN, and LogisticRegression with **GridSearchCV**.
We also add **PCA** to reduce dimensionality and speed up fitting.
"""

SK_DIR = Path(WORK_DIR,"models","sklearn"); SK_DIR.mkdir(parents=True, exist_ok=True)

grids = {
    'svm_rbf': (SVC(probability=True), {
        'clf__C': [0.5, 1, 2],
        'clf__gamma': ['scale', 0.01, 0.001]
    }),
    'random_forest': (RandomForestClassifier(n_estimators=300, random_state=SEED), {
        'clf__max_depth': [None, 10, 20],
        'clf__min_samples_split': [2, 5]
    }),
    'knn': (KNeighborsClassifier(), {
        'clf__n_neighbors': [3, 5, 7]
    }),
    'logreg': (LogisticRegression(max_iter=300), {
        'clf__C': [0.5, 1.0, 2.0]
    })
}

def fit_eval_model(name, est, param_grid, Xtr, ytr, Xv, yv, Xte, yte):
    pipe = SkPipeline([('scaler', StandardScaler()),
                       ('pca', PCA(n_components=256, random_state=SEED)),
                       ('clf', est)])
    search = GridSearchCV(pipe, param_grid=param_grid, cv=3, n_jobs=-1, scoring='f1_weighted', verbose=1)
    search.fit(Xtr, ytr)
    best = search.best_estimator_
    with open(SK_DIR / f"{name}.pkl", "wb") as f:
        pickle.dump(best, f)  # use pickle as per requirement

    def eval_split(X, y, split_name):
        y_pred = best.predict(X)
        acc = accuracy_score(y, y_pred)
        pr, rc, f1, _ = precision_recall_fscore_support(y, y_pred, average='weighted', zero_division=0)
        print(f"[{name.upper()}][{split_name}] acc={acc:.4f} f1={f1:.4f}")
        return {"accuracy": acc, "precision_weighted": pr, "recall_weighted": rc, "f1_weighted": f1}

    res = {
        "best_params": search.best_params_,
        "val":  eval_split(Xv,  yv,  "VAL"),
        "test": eval_split(Xte, yte, "TEST")
    }
    # Confusion matrix (test)
    y_pred_te = best.predict(Xte)
    fig, ax = plt.subplots(figsize=(6,6))
    ConfusionMatrixDisplay.from_predictions(yte, y_pred_te, display_labels=class_names, ax=ax, xticks_rotation=45)
    plt.title(f"{name} â€” Test Confusion Matrix")
    plt.tight_layout()
    fig_path = Path(WORK_DIR,"eda",f"cm_{name}.png")
    plt.savefig(fig_path); plt.close(fig)
    return res

results = {}
for name, (est, grid) in grids.items():
    results[name] = fit_eval_model(name, est, grid, Xtr, y_train, Xv, y_val, Xte, y_test)

# Save per-model metrics for the web app
metrics_path = Path(WORK_DIR,"metrics","model_metrics.json")
with open(metrics_path, "w") as f:
    json.dump(results, f, indent=2)
print("Saved metrics to:", metrics_path)

"""## Step 7 â€” CNN Detailed Evaluation & Persistence
We compute full metrics and confusion matrix for the CNN and store them for the web app.
"""

# Predict and metrics
y_true_te = []
y_pred_te = []
for bx, by in test_ds:
    yp = custom.predict(bx, verbose=0)
    y_pred_te.extend(np.argmax(yp, axis=1))
    y_true_te.extend(by.numpy())
y_true_te = np.array(y_true_te); y_pred_te = np.array(y_pred_te)

acc = accuracy_score(y_true_te, y_pred_te)
pr, rc, f1, _ = precision_recall_fscore_support(y_true_te, y_pred_te, average='weighted', zero_division=0)
print(f"[CNN][TEST] acc={acc:.4f} f1={f1:.4f}")
print(classification_report(y_true_te, y_pred_te, target_names=class_names))

# Confusion matrix
fig, ax = plt.subplots(figsize=(6,6))
ConfusionMatrixDisplay.from_predictions(y_true_te, y_pred_te, display_labels=class_names, ax=ax, xticks_rotation=45)
plt.title("CNN â€” Test Confusion Matrix")
plt.tight_layout()
plt.savefig(Path(WORK_DIR,"eda","cm_cnn.png")); plt.close(fig)

# Save CNN metrics
metrics_full = {}
metrics_json_path = Path(WORK_DIR,"metrics","model_metrics.json")
if Path(metrics_json_path).exists():
    with open(metrics_json_path, "r") as f:
        metrics_full = json.load(f)
metrics_full["cnn_custom"] = {"test": {"accuracy": float(acc), "precision_weighted": float(pr),
                                       "recall_weighted": float(rc), "f1_weighted": float(f1)}}
with open(metrics_json_path, "w") as f:
    json.dump(metrics_full, f, indent=2)
print("Updated metrics with CNN:", metrics_json_path)

# Save label encoder for app
with open(Path(WORK_DIR,"models","label_encoder.pkl"), "wb") as f:
    pickle.dump(le, f)

"""## Step 8/9 â€” Save Artifacts & GitHub Checklist
- Save models (pickle for traditional, `.keras` for CNN)
- Save metrics JSON
- Upload this notebook, `app/streamlit_app.py`, and README.md to GitHub

## Done â€” Next Step: Run the **Streamlit app** locally or deploy to Streamlit Community Cloud.
"""